{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(1000, 227, 227) (1000,)\n",
      "(200, 227, 227) (200,)\n",
      "(80, 227, 227) (80,)\n"
=======
      "['GrayscaleNormalizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'filter', 'np']\n",
      "<class 'filters.filter.GrayscaleNormalizer'>\n"
>>>>>>> 4d638636106cfd9f369aa04b7c7a2e8d90dde108
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab as pl\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import helper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 227, 227) (1000,)\n",
      "(200, 227, 227) (200,)\n",
      "(80, 227, 227) (80,)\n"
     ]
    }
   ],
   "source": [
    "FOLDER_PATH = '..'\n",
    "\n",
    "# random\n",
    "train_size = 1000\n",
    "test_size = 100\n",
    "val_size = 40\n",
    "\n",
    "img_range = np.arange(1, 20001)\n",
    "X_train_pos_idx, X_test_pos_idx, X_val_pos_idx = helper.get_random_indices(img_range, train_size, test_size, val_size)\n",
    "X_train_neg_idx, X_test_neg_idx, X_val_neg_idx = helper.get_random_indices(img_range, train_size, test_size, val_size)\n",
    "\n",
    "X_train, Y_train = helper.get_concrete_data(X_train_pos_idx, X_train_neg_idx, path = FOLDER_PATH)\n",
    "X_test , Y_test  = helper.get_concrete_data(X_test_pos_idx, X_test_neg_idx, path = FOLDER_PATH)\n",
    "X_val  , Y_val   = helper.get_concrete_data(X_val_pos_idx, X_val_neg_idx, path = FOLDER_PATH)\n",
    "\n",
    "print( X_train.shape, Y_train.shape )\n",
    "print( X_test.shape , Y_test.shape  )\n",
    "print( X_val.shape  , Y_val.shape   )\n",
    "\n",
    "from filters import GrayscaleNormalizer\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
<<<<<<< HEAD
    "tf.logging.set_verbosity(tf.logging.INFO)"
=======
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# automatically reload imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
>>>>>>> 4d638636106cfd9f369aa04b7c7a2e8d90dde108
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess the image by adding filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(X, labels, is_training):\n",
    "\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(X, [-1, 227, 227, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 227, 227, 1]\n",
    "  # Output Tensor Shape: [batch_size, 227, 227, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    bn1act = tf.layers.batch_normalization(inputs=conv1, training=is_training)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 227, 227, 32]\n",
    "  # Output Tensor Shape: [batch_size, 113, 113, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=bn1act, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x5 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 113, 113, 32]\n",
    "  # Output Tensor Shape: [batch_size, 113, 113, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    bn2act = tf.layers.batch_normalization(inputs=conv2, training=is_training)\n",
    "\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 113, 113, 64]\n",
    "  # Output Tensor Shape: [batch_size, 56, 56, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=bn2act, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 56, 56, 64]\n",
    "  # Output Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 56 * 56 * 64])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    bn3act = tf.layers.batch_normalization(inputs=dense, training=is_training)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=bn3act, rate=0.4, training=is_training)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 2]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2,activation=None)\n",
    "    return logits\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#   predictions = {\n",
    "#       # Generate predictions (for PREDICT and EVAL mode)\n",
    "#       \"classes\": tf.argmax(input=logits, axis=1),\n",
    "#       # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "#       # `logging_hook`.\n",
    "#       \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "#   }\n",
    "#   if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "#     return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "#   loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "#   # Configure the Training Op (for TRAIN mode)\n",
    "#   if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "#     train_op = optimizer.minimize(\n",
    "#         loss=loss,\n",
    "#         global_step=tf.train.get_global_step())\n",
    "#     return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "#   # Add evaluation metrics (for EVAL mode)\n",
    "#   eval_metric_ops = {\n",
    "#       \"accuracy\": tf.metrics.accuracy(\n",
    "#           labels=labels, predictions=predictions[\"classes\"])}\n",
    "#   return tf.estimator.EstimatorSpec(\n",
    "#       mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "#   train_data = mnist.train.images  # Returns np.array\n",
    "#   train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "#   eval_data = mnist.test.images  # Returns np.array\n",
    "#   eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "#   # Create the Estimator\n",
    "#   mnist_classifier = tf.estimator.Estimator(\n",
    "#       model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n",
    "#   # Set up logging for predictions\n",
    "#   # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "#   tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "#   logging_hook = tf.train.LoggingTensorHook(\n",
    "#       tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "#   # Train the model\n",
    "#   train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#       x={\"x\": train_data},\n",
    "#       y=train_labels,\n",
    "#       batch_size=100,\n",
    "#       num_epochs=None,\n",
    "#       shuffle=True)\n",
    "#   mnist_classifier.train(\n",
    "#       input_fn=train_input_fn,\n",
    "#       steps=20000,\n",
    "#       hooks=[logging_hook])\n",
    "\n",
    "#   # Evaluate the model and print results\n",
    "#   eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#       x={\"x\": eval_data},\n",
    "#       y=eval_labels,\n",
    "#       num_epochs=1,\n",
    "#       shuffle=False)\n",
    "#   eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "#   print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss, correct_prediction, accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        # keep track of accuracy\n",
    "        correct = 0\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            corr = np.array(corr).astype(np.float32)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "            \n",
    "    return total_loss,total_correct,losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with 1000 training data entries 10 epoch, validate with 50 data entries and test with 100 data entries:\n",
    "\n",
    "Validation\n",
    "\n",
    "Epoch 1, Overall loss = 0.247 and accuracy of 0.94\n",
    "\n",
    "Test\n",
    "\n",
    "Epoch 1, Overall loss = 0.298 and accuracy of 0.89\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "## add filter and train again (with maybe 500 data entries for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dominic\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getTPTNRate(l,p):#label, predictions\n",
    "    tn, fp, fn, tp = confusion_matrix(l, p).ravel()\n",
    "    tpr = float(tp)/(float(tp) + float(fn))\n",
    "    tnr=float(tn)/(float(tn) + float(fp))\n",
    "    return tpr,tnr\n",
    "\n",
    "\n",
    "def run_model_TF(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    predictions=tf.argmax(predict,1)\n",
    "    correct_prediction = tf.equal(predictions, y)#array of true and false\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "    actuals=y\n",
    "    ones_like_actuals = tf.ones_like(actuals)#all elements set to 1\n",
    "    zeros_like_actuals = tf.zeros_like(actuals) #all elements set to 0\n",
    "    ones_like_predictions = tf.ones_like(predictions) #all elements set to 1\n",
    "    zeros_like_predictions = tf.zeros_like(predictions) #all elements set to 0\n",
    "    \n",
    "    tp_op = tf.count_nonzero(predictions * actuals)\n",
    "    tn_op = tf.count_nonzero((predictions - 1) * (actuals - 1))\n",
    "    fp_op = tf.count_nonzero(predictions * (actuals - 1))\n",
    "    fn_op = tf.count_nonzero((predictions - 1) * actuals)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss, tp_op, tn_op, fp_op, fn_op, correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "#     tprs=[]\n",
    "    tnrs=[]\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        total_tp=0\n",
    "        total_tn=0\n",
    "        total_fp=0\n",
    "        total_fn=0\n",
    "        correct = 0\n",
    "\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, tp, tn, fp, fn, corr,_= session.run(variables,feed_dict=feed_dict) #last variable will be automatically be None \n",
    "\n",
    "\n",
    "#             if fn is None:\n",
    "#                 fn=actual_batch_size-tp-tn-fp\n",
    "   \n",
    "            print(tp,tn,fp,fn)\n",
    "            total_tp+=tp\n",
    "            total_tn+=tn\n",
    "            total_fp+=fp \n",
    "            total_fn+=fn\n",
    "            tpr = float(tp)/(float(tp) + float(fn))\n",
    "#             fpr = float(fp)/(float(tp) + float(fn))\n",
    "            tnr=float(tn)/(float(tn) + float(fp))\n",
    "#             accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "            \n",
    "#             recall = tpr\n",
    "#             precision = float(tp)/(float(tp) + float(fp))\n",
    "#             f1_score = (2.0 * (precision * recall)) / (precision + recall)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "#             tprs.append(tpr)\n",
    "#             tnrs.append(tnr)\n",
    "#             print(len(tprs),tprs)\n",
    "#             print(len(tnrs),tnrs)\n",
    "#             print(corr)\n",
    "            corr = np.array(corr).astype(np.float32)\n",
    "            \n",
    "            correct += np.sum(corr)\n",
    "\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g},tpr of {2:.2g},tnr of {3:.2g} and accuracy of {4:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,tpr,tnr,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_tpr = total_tp/(total_tp+total_fn)\n",
    "        total_tnr=total_tn/(total_tn+total_fp)\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        \n",
    "\n",
    "\n",
    "        print(\"Epoch {0}, Overall loss = {1:.3g} tpr of {2:.3g},tnr of {3:.3g} and accuracy of {4:.3g}\"\\\n",
    "              .format(e+1,total_loss,total_tpr,total_tnr,total_correct))\n",
    "    \n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "            \n",
    "    return total_loss,total_tpr,total_tnr,losses\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 227, 227])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out = cnn_model_fn(X,y,is_training)\n",
    "total_loss = tf.losses.softmax_cross_entropy(logits=y_out, onehot_labels=tf.one_hot(y,2))\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) #adam\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 227, 227) (1000,)\n",
      "(80, 227, 227) (80,)\n",
      "(200, 227, 227) (200,)\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# #read the data\n",
    "# #train 500 negatives and 500 positives\n",
    "# #use 50 negatives and 50 positives to validate\n",
    "# #100 negatives and 100 positives to test\n",
    "# X_train=[]\n",
    "# Y_train=[]\n",
    "# X_test=[]\n",
    "# Y_test=[] \n",
    "# X_val=[]\n",
    "# Y_val=[]  \n",
    "# for i in range(1,501):\n",
    "#     j  = i + 1600\n",
    "#     txt = '../Negative/0'+str(j).zfill(4)+'.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_train.append(img)\n",
    "#     Y_train.append(0)\n",
    "    \n",
    "# for i in range(1,501):\n",
    "#     j  = i + 1600\n",
    "#     txt = '../Positive/0'+str(j).zfill(4)+'.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_train.append(img)\n",
    "#     Y_train.append(1)\n",
=======
    "import helper\n",
>>>>>>> 4d638636106cfd9f369aa04b7c7a2e8d90dde108
    "\n",
    "FOLDER_PATH = 'C://Users//Dominic//Desktop//Concrete Crack Images for Classification'\n",
    "\n",
<<<<<<< HEAD
    "    \n",
    "# for i in range(1,41):\n",
    "#     j  = i + 500\n",
    "#     txt = '../Negative/1'+str(i).zfill(4)+'.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_val.append(img)\n",
    "#     Y_val.append(0)\n",
    "#     txt = '../Positive/1'+str(i).zfill(4)+'_1.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_val.append(img)\n",
    "#     Y_val.append(1)\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(1,101):\n",
    "#     j  = i + 1000\n",
    "#     txt = '../Negative/1'+str(j).zfill(4)+'.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_test.append(img)\n",
    "#     Y_test.append(0)\n",
    "\n",
    "#     txt = '../Positive/1'+str(j).zfill(4)+'_1.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_test.append(img)\n",
    "#     Y_test.append(1)\n",
    "\n",
    "\n",
    "\n",
    "# X_test=np.asarray(X_test)\n",
    "# Y_test=np.asarray(Y_test)    \n",
    "# X_val=np.asarray(X_val)\n",
    "# Y_val=np.asarray(Y_val)\n",
    "# X_train=np.asarray(X_train)\n",
    "# Y_train=np.asarray(Y_train)"
=======
    "# fixed from range\n",
    "X_train, Y_train = helper.get_concrete_data(range(1601, 2101), range(1601, 2101), path = FOLDER_PATH)\n",
    "X_test , Y_test  = helper.get_concrete_data(range(501 , 541 ), range(501 , 541 ), path = FOLDER_PATH)\n",
    "X_val  , Y_val   = helper.get_concrete_data(range(1001, 1101), range(1001, 1101), path = FOLDER_PATH)\n",
    "\n",
    "print( X_train.shape, Y_train.shape )\n",
    "print( X_test.shape , Y_test.shape  )\n",
    "print( X_val.shape  , Y_val.shape   )"
>>>>>>> 4d638636106cfd9f369aa04b7c7a2e8d90dde108
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "18 28 3 15\n",
      "Iteration 0: with minibatch training loss = 0.703,tpr of 0.55,tnr of 0.9 and accuracy of 0.72\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model_TF(sess,y_out,mean_loss,X_train,Y_train,3,64,100,train_step,True)\n",
    "\n",
    "mypredictions=tf.argmax(y_out,1)\n",
    "print('Validation')\n",
    "predicted_val=mypredictions.eval(feed_dict={X:X_val,is_training:True}, session=sess)\n",
    "print(predicted_val)\n",
    "print(Y_val)\n",
    "tpr_val,tnr_val=getTPTNRate(Y_val,predicted_val)\n",
    "print(\"true positive rate\",tpr_val,\"true negative rate\",tnr_val)\n",
    "\n",
    "print('Test')\n",
    "predicted_test=mypredictions.eval(feed_dict={X:X_test,is_training:True}, session=sess)\n",
    "print(predicted_test)\n",
    "print(Y_test)\n",
    "tpr_test,tnr_test=getTPTNRate(Y_test,predicted_test)\n",
    "print(\"true positive rate\",tpr_test,\"true negative rate\",tnr_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation:true positive rate 0.9 true negative rate 0.95\n",
    "\n",
    "test:true positive rate 0.91 true negative rate 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "read the data\n",
    "train 500 negatives and 500 positives\n",
    "use 50 negatives and 50 positives to validate\n",
    "100 negatives and 100 positives to test\n",
    "'''\n",
    "\n",
    "# X_train=[]\n",
    "# Y_train=[]\n",
    "# X_test=[]\n",
    "# Y_test=[] \n",
    "# X_val=[]\n",
    "# Y_val=[]  \n",
    "# for i in range(1,501):\n",
    "#     j  = i + 1600\n",
    "#     txt = '../Negative/0'+str(j).zfill(4)+'.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_train.append(img)\n",
    "#     Y_train.append(0)\n",
    "    \n",
    "# for i in range(1,501):\n",
    "#     j  = i + 1600\n",
    "#     txt = '../Positive/0'+str(j).zfill(4)+'.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_train.append(img)\n",
    "#     Y_train.append(1)\n",
    "\n",
    "\n",
    "    \n",
    "# for i in range(1,41):\n",
    "#     j  = i + 500\n",
    "#     txt = '../Negative/1'+str(i).zfill(4)+'.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_val.append(img)\n",
    "#     Y_val.append(0)\n",
    "#     txt = '../Positive/1'+str(i).zfill(4)+'_1.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_val.append(img)\n",
    "#     Y_val.append(1)\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(1,101):\n",
    "#     j  = i + 1000\n",
    "#     txt = '../Negative/1'+str(j).zfill(4)+'.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_test.append(img)\n",
    "#     Y_test.append(0)\n",
    "\n",
    "#     txt = '../Positive/1'+str(j).zfill(4)+'_1.jpg'\n",
    "#     img = cv2.imread(txt, 0).astype(np.float32)\n",
    "#     X_test.append(img)\n",
    "#     Y_test.append(1)\n",
    "\n",
    "\n",
    "\n",
    "# X_test=np.asarray(X_test)\n",
    "# Y_test=np.asarray(Y_test)    \n",
    "# X_val=np.asarray(X_val)\n",
    "# Y_val=np.asarray(Y_val)\n",
    "# X_train=np.asarray(X_train)\n",
    "# Y_train=np.asarray(Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
