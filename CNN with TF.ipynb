{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab as pl\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import helper.get_image as gi\n",
    "from filters import GrayscaleNormalizer\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# automatically reload imported modules\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 227, 227) (2000,)\n",
      "(80, 227, 227) (80,)\n",
      "(202, 227, 227) (202,)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "FOLDER_PATH = '..'\n",
    "# random\n",
    "train_size = 1000\n",
    "test_size = 100\n",
    "val_size = 40\n",
    "\n",
    "# img_range = np.arange(1, 20001)\n",
    "# X_train_pos_idx, X_test_pos_idx, X_val_pos_idx = gi.get_random_indices(img_range, train_size, test_size, val_size)\n",
    "# X_train_neg_idx, X_test_neg_idx, X_val_neg_idx = gi.get_random_indices(img_range, train_size, test_size, val_size)\n",
    "\n",
    "# X_train, Y_train = gi.get_concrete_data(X_train_pos_idx, X_train_neg_idx, path = FOLDER_PATH)\n",
    "# X_test , Y_test  = gi.get_concrete_data(X_test_pos_idx, X_test_neg_idx, path = FOLDER_PATH)\n",
    "# X_val  , Y_val   = gi.get_concrete_data(X_val_pos_idx, X_val_neg_idx, path = FOLDER_PATH)\n",
    "\n",
    "# print( X_train.shape, Y_train.shape )\n",
    "# print( X_test.shape , Y_test.shape  )\n",
    "# print( X_val.shape  , Y_val.shape   )\n",
    "#fixed from range\n",
    "# X_train, Y_train = gi.get_concrete_data(range(1601, 3601), range(1601, 3601), path = FOLDER_PATH)\n",
    "# X_test , Y_test  = gi.get_concrete_data(range(501 , 541 ), range(501 , 541 ), path = FOLDER_PATH)\n",
    "# X_val  , Y_val   = gi.get_concrete_data(range(1001, 1101), range(1001, 1101), path = FOLDER_PATH)\n",
    "# X_test2 , Y_test2  = gi.get_concrete_data(range(542 , 1000 ), range(542 , 1000 ), path = FOLDER_PATH)\n",
    "\n",
    "X_train, Y_train = gi.get_concrete_data(range(0, 1000), range(0, 1000), path = FOLDER_PATH)\n",
    "X_test , Y_test  = gi.get_concrete_data(range(1000 , 1040 ), range(1000 , 1040 ), path = FOLDER_PATH)\n",
    "X_val  , Y_val   = gi.get_concrete_data(range(1040, 1141), range(1040, 1141), path = FOLDER_PATH)\n",
    "# X_test2 , Y_test2  = gi.get_concrete_data(range(542 , 1000 ), range(542 , 1000 ), path = FOLDER_PATH)\n",
    "\n",
    "print( X_train.shape, Y_train.shape )\n",
    "print( X_test.shape , Y_test.shape  )\n",
    "print( X_val.shape  , Y_val.shape   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTPTNRate(l,p):#label, predictions\n",
    "    tn, fp, fn, tp = confusion_matrix(l, p).ravel()\n",
    "    tpr = float(tp)/(float(tp) + float(fn))\n",
    "    tnr=float(tn)/(float(tn) + float(fp))\n",
    "    return tpr,tnr\n",
    "# this is for training \n",
    "def run_model_TF(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    predictions=tf.argmax(predict,1)\n",
    "    correct_prediction = tf.equal(predictions, y)#array of true and false\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "    actuals=y\n",
    "    ones_like_actuals = tf.ones_like(actuals)#all elements set to 1\n",
    "    zeros_like_actuals = tf.zeros_like(actuals) #all elements set to 0\n",
    "    ones_like_predictions = tf.ones_like(predictions) #all elements set to 1\n",
    "    zeros_like_predictions = tf.zeros_like(predictions) #all elements set to 0\n",
    "    \n",
    "    tp_op = tf.count_nonzero(predictions * actuals)\n",
    "    tn_op = tf.count_nonzero((predictions - 1) * (actuals - 1))\n",
    "    fp_op = tf.count_nonzero(predictions * (actuals - 1))\n",
    "    fn_op = tf.count_nonzero((predictions - 1) * actuals)\n",
    " \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss, tp_op, tn_op, fp_op, fn_op, correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        total_tp=0\n",
    "        total_tn=0\n",
    "        total_fp=0\n",
    "        total_fn=0\n",
    "        correct = 0\n",
    "\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, tp, tn, fp, fn, corr,_= session.run(variables,feed_dict=feed_dict) #last variable will be automatically be None \n",
    "\n",
    "\n",
    "            print(tp,tn,fp,fn)\n",
    "            total_tp+=tp\n",
    "            total_tn+=tn\n",
    "            total_fp+=fp \n",
    "            total_fn+=fn\n",
    "            tpr = float(tp)/(float(tp) + float(fn))\n",
    "#             fpr = float(fp)/(float(tp) + float(fn))\n",
    "            tnr=float(tn)/(float(tn) + float(fp))\n",
    "#             accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "            \n",
    "#             recall = tpr\n",
    "#             precision = float(tp)/(float(tp) + float(fp))\n",
    "#             f1_score = (2.0 * (precision * recall)) / (precision + recall)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "\n",
    "            corr = np.array(corr).astype(np.float32)\n",
    "            \n",
    "            correct += np.sum(corr)\n",
    "\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g},tpr of {2:.2g},tnr of {3:.2g} and accuracy of {4:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,tpr,tnr,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_tpr = total_tp/(total_tp+total_fn)\n",
    "        total_tnr=total_tn/(total_tn+total_fp)\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        \n",
    "\n",
    "\n",
    "        print(\"Epoch {0}, Overall loss = {1:.3g} tpr of {2:.3g},tnr of {3:.3g} and accuracy of {4:.3g}\"\\\n",
    "              .format(e+1,total_loss,total_tpr,total_tnr,total_correct))\n",
    "    \n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "            \n",
    "    return total_loss,total_tpr,total_tnr,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "8 12 19 25\n",
      "Iteration 0: with minibatch training loss = 1.33,tpr of 0.24,tnr of 0.39 and accuracy of 0.31\n",
      "24 22 11 7\n",
      "21 28 9 6\n",
      "21 28 1 14\n",
      "20 27 0 17\n",
      "28 24 1 11\n",
      "24 33 0 7\n",
      "23 35 1 5\n",
      "24 28 0 12\n",
      "24 35 1 4\n",
      "26 27 0 11\n",
      "30 27 0 7\n",
      "23 33 0 8\n",
      "20 36 2 6\n",
      "25 36 0 3\n",
      "23 29 2 10\n",
      "28 27 0 9\n",
      "28 29 0 7\n",
      "25 33 0 6\n",
      "28 33 1 2\n",
      "24 35 2 3\n",
      "29 27 0 8\n",
      "26 33 2 3\n",
      "25 37 0 2\n",
      "29 31 1 3\n",
      "29 33 0 2\n",
      "30 29 1 4\n",
      "32 30 0 2\n",
      "31 26 0 7\n",
      "28 29 1 6\n",
      "31 31 1 1\n",
      "28 31 1 4\n",
      "25 35 0 4\n",
      "24 37 1 2\n",
      "29 35 0 0\n",
      "29 32 1 2\n",
      "26 33 1 4\n",
      "28 34 0 2\n",
      "34 26 1 3\n",
      "32 25 1 6\n",
      "22 40 0 2\n",
      "31 27 1 5\n",
      "32 28 0 4\n",
      "25 37 0 2\n",
      "28 33 0 3\n",
      "27 35 0 2\n",
      "32 29 0 3\n",
      "33 27 1 3\n",
      "33 30 0 1\n",
      "26 36 2 0\n",
      "31 28 1 4\n",
      "36 26 0 2\n",
      "31 32 0 1\n",
      "31 31 0 2\n",
      "29 34 0 1\n",
      "29 27 0 8\n",
      "27 36 1 0\n",
      "32 31 0 1\n",
      "32 30 0 2\n",
      "26 37 1 0\n",
      "30 32 0 2\n",
      "26 33 2 3\n",
      "13 19 0 0\n",
      "Epoch 1, Overall loss = 0.255 tpr of 0.852,tnr of 0.965 and accuracy of 0.908\n",
      "31 30 2 1\n",
      "29 30 0 5\n",
      "33 27 0 4\n",
      "31 32 0 1\n",
      "29 33 0 2\n",
      "29 35 0 0\n",
      "31 32 0 1\n",
      "28 36 0 0\n",
      "29 33 1 1\n",
      "30 33 0 1\n",
      "29 34 0 1\n",
      "31 31 1 1\n",
      "28 35 1 0\n",
      "31 33 0 0\n",
      "31 31 0 2\n",
      "30 32 0 2\n",
      "33 30 0 1\n",
      "35 27 0 2\n",
      "38 19 0 7\n",
      "34 30 0 0\n",
      "23 39 1 1\n",
      "28 35 0 1\n",
      "30 33 0 1\n",
      "32 30 1 1\n",
      "35 26 0 3\n",
      "31 32 0 1\n",
      "33 29 0 2\n",
      "35 28 0 1\n",
      "31 31 0 2\n",
      "28 35 1 0\n",
      "36 24 0 4\n",
      "31 30 0 3\n",
      "26 38 0 0\n",
      "28 36 0 0\n",
      "31 33 0 0\n",
      "31 32 0 1\n",
      "35 29 0 0\n",
      "34 29 1 0\n",
      "Iteration 100: with minibatch training loss = 0.044,tpr of 1,tnr of 0.97 and accuracy of 0.98\n",
      "32 29 0 3\n",
      "24 35 1 4\n",
      "33 30 0 1\n",
      "26 36 1 1\n",
      "30 32 0 2\n",
      "33 30 1 0\n",
      "34 29 0 1\n",
      "26 38 0 0\n",
      "25 39 0 0\n",
      "35 27 1 1\n",
      "27 36 1 0\n",
      "33 30 0 1\n",
      "30 33 0 1\n",
      "31 33 0 0\n",
      "34 30 0 0\n",
      "32 32 0 0\n",
      "35 29 0 0\n",
      "29 33 1 1\n",
      "32 31 0 1\n",
      "27 37 0 0\n",
      "29 35 0 0\n",
      "32 31 0 1\n",
      "26 38 0 0\n",
      "38 26 0 0\n",
      "16 14 0 2\n",
      "Epoch 2, Overall loss = 0.331 tpr of 0.964,tnr of 0.993 and accuracy of 0.978\n",
      "30 34 0 0\n",
      "31 32 0 1\n",
      "30 34 0 0\n",
      "33 28 0 3\n",
      "31 32 0 1\n",
      "30 32 0 2\n",
      "31 33 0 0\n",
      "35 26 0 3\n",
      "30 33 1 0\n",
      "26 38 0 0\n",
      "36 27 0 1\n",
      "31 33 0 0\n",
      "32 32 0 0\n",
      "32 32 0 0\n",
      "31 27 0 6\n",
      "31 33 0 0\n",
      "33 31 0 0\n",
      "39 24 0 1\n",
      "30 33 1 0\n",
      "31 33 0 0\n",
      "26 37 1 0\n",
      "33 28 1 2\n",
      "29 35 0 0\n",
      "30 34 0 0\n",
      "38 24 0 2\n",
      "30 34 0 0\n",
      "27 35 2 0\n",
      "33 31 0 0\n",
      "36 26 0 2\n",
      "27 36 0 1\n",
      "36 27 0 1\n",
      "25 39 0 0\n",
      "29 33 1 1\n",
      "38 24 0 2\n",
      "34 29 1 0\n",
      "37 26 1 0\n",
      "31 31 0 2\n",
      "31 33 0 0\n",
      "33 31 0 0\n",
      "25 37 1 1\n",
      "35 29 0 0\n",
      "32 32 0 0\n",
      "32 31 0 1\n",
      "24 40 0 0\n",
      "34 29 0 1\n",
      "29 35 0 0\n",
      "34 30 0 0\n",
      "28 36 0 0\n",
      "28 35 1 0\n",
      "27 37 0 0\n",
      "33 31 0 0\n",
      "29 34 1 0\n",
      "33 31 0 0\n",
      "33 30 0 1\n",
      "29 35 0 0\n",
      "29 34 0 1\n",
      "34 30 0 0\n",
      "33 29 0 2\n",
      "30 34 0 0\n",
      "30 32 0 2\n",
      "31 31 1 1\n",
      "35 29 0 0\n",
      "16 16 0 0\n",
      "Epoch 3, Overall loss = 0.371 tpr of 0.98,tnr of 0.994 and accuracy of 0.987\n",
      "30 34 0 0\n",
      "32 31 1 0\n",
      "36 28 0 0\n",
      "34 30 0 0\n",
      "33 30 1 0\n",
      "35 29 0 0\n",
      "33 31 0 0\n",
      "29 35 0 0\n",
      "34 29 0 1\n",
      "31 33 0 0\n",
      "26 36 2 0\n",
      "31 33 0 0\n",
      "Iteration 200: with minibatch training loss = 0.0172,tpr of 1,tnr of 1 and accuracy of 1\n",
      "37 27 0 0\n",
      "35 29 0 0\n",
      "28 36 0 0\n",
      "28 36 0 0\n",
      "27 37 0 0\n",
      "30 34 0 0\n",
      "34 29 0 1\n",
      "33 31 0 0\n",
      "33 30 0 1\n",
      "26 37 1 0\n",
      "38 26 0 0\n",
      "27 37 0 0\n",
      "30 33 0 1\n",
      "37 27 0 0\n",
      "35 29 0 0\n",
      "32 32 0 0\n",
      "31 33 0 0\n",
      "31 33 0 0\n",
      "36 27 1 0\n",
      "30 34 0 0\n",
      "27 37 0 0\n",
      "39 25 0 0\n",
      "40 24 0 0\n",
      "32 32 0 0\n",
      "31 33 0 0\n",
      "39 24 0 1\n",
      "34 30 0 0\n",
      "32 32 0 0\n",
      "33 31 0 0\n",
      "36 27 0 1\n",
      "33 31 0 0\n",
      "28 36 0 0\n",
      "28 36 0 0\n",
      "29 35 0 0\n",
      "23 40 1 0\n",
      "27 37 0 0\n",
      "30 34 0 0\n",
      "22 42 0 0\n",
      "31 32 1 0\n",
      "32 32 0 0\n",
      "32 32 0 0\n",
      "37 27 0 0\n",
      "29 35 0 0\n",
      "37 27 0 0\n",
      "31 33 0 0\n",
      "36 28 0 0\n",
      "32 32 0 0\n",
      "36 26 0 2\n",
      "27 37 0 0\n",
      "28 36 0 0\n",
      "18 13 0 1\n",
      "Epoch 4, Overall loss = 0.394 tpr of 0.996,tnr of 0.996 and accuracy of 0.996\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAHwCAYAAADuJ7gwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEmEKhUKhUCgUUUCJMIVCoVAoFIoo8P8BFHhXTAEjeAgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1058fdeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../trained_model2/cnn_model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN_model:\n",
    "    def __init__(self,X_train,Y_train):\n",
    "        self.X_train=X_train\n",
    "        self.Y_train=Y_train\n",
    "        \n",
    "    def cnn_model_fn(self,X, labels, is_training):\n",
    "        #1-layer model: conv-pool-fc-fc-softmax with batch normalization \n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "        input_layer = tf.reshape(X, [-1, 227, 227, 1])\n",
    "\n",
    "        conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "        bn1act = tf.layers.batch_normalization(inputs=conv1, training=is_training)\n",
    "                #add dropout \n",
    "        dropout_conv1 = tf.layers.dropout(\n",
    "      inputs=bn1act, rate=0.3, training=is_training)\n",
    "\n",
    "\n",
    "\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=dropout_conv1, pool_size=[2, 2], strides=2)\n",
    "        conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "                #add dropout \n",
    "        dropout_conv2 = tf.layers.dropout(\n",
    "      inputs=conv2, rate=0.3, training=is_training)\n",
    "        bn2act = tf.layers.batch_normalization(inputs=dropout_conv2, training=is_training)\n",
    "\n",
    "\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=bn2act, pool_size=[2, 2], strides=2)\n",
    "       \n",
    "  # Flatten tensor into a batch of vectors\n",
    "        pool2_flat = tf.reshape(pool2, [-1, 56 * 56 * 32])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 128 neurons\n",
    "        dense = tf.layers.dense(inputs=pool2_flat, units=128, activation=tf.nn.relu)\n",
    "        bn2act = tf.layers.batch_normalization(inputs=dense, training=is_training)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept to avoid overfitting\n",
    "        dropout = tf.layers.dropout(\n",
    "      inputs=bn2act, rate=0.3, training=is_training)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 128]\n",
    "  # Output Tensor Shape: [batch_size, 2]\n",
    "        logits = tf.layers.dense(inputs=dropout, units=2,activation=None,name=\"logits\")\n",
    "        return logits\n",
    "\n",
    "\n",
    "#load model\n",
    "model=CNN_model(X_train,Y_train)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 227, 227],name=\"X\")\n",
    "y = tf.placeholder(tf.int64, [None],name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool,name=\"training\")\n",
    "\n",
    "y_out = model.cnn_model_fn(X,y,is_training)\n",
    "# print(y_out)\n",
    "# print(is_training)\n",
    "total_loss = tf.losses.softmax_cross_entropy(logits=y_out, onehot_labels=tf.one_hot(y,2))\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) #adam\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "print('Training')\n",
    "run_model_TF(sess,y_out,mean_loss,X_train,Y_train,4,64,100,train_step,True)\n",
    "#save the model\n",
    "#the training is over 4000 data(2000 pos 2000 neg) over 5 epoch\n",
    "saver.save(sess, '../trained_model2/cnn_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../trained_model1/cnn_model\n",
      "Test\n",
      "[0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1\n",
      " 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
      " 0 1 0 1 1 0]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "true positive rate 0.7 true negative rate 0.475\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "true positive rate 1.0 true negative rate 0.0\n",
      "Test\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1\n",
      " 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "true positive rate 0.84 true negative rate 0.46\n"
     ]
    }
   ],
   "source": [
    "#this is to load the saved trained model\n",
    "\n",
    "sess=tf.Session()    \n",
    "#retrieve the model\n",
    "saver = tf.train.import_meta_graph('../trained_model2/cnn_model.meta')\n",
    "saver.restore(sess, '../trained_model2/cnn_model')\n",
    "graph = tf.get_default_graph()\n",
    "stored_y_out = graph.get_tensor_by_name(\"logits/BiasAdd:0\")\n",
    "X = graph.get_tensor_by_name(\"X:0\")\n",
    "is_training=graph.get_tensor_by_name(\"training:0\")\n",
    "\n",
    "mypredictions=tf.argmax(stored_y_out,1)\n",
    "print('Test')\n",
    "#predicted result based on input\n",
    "predicted_test=mypredictions.eval(feed_dict={X:X_test,is_training:False}, session=sess)\n",
    "print(predicted_test)\n",
    "print(Y_test)\n",
    "tpr_test,tnr_test=getTPTNRate(Y_test,predicted_test)\n",
    "print(\"true positive rate\",tpr_test,\"true negative rate\",tnr_test)\n",
    "\n",
    "predicted_test2=mypredictions.eval(feed_dict={X:X_test2,is_training:False}, session=sess)\n",
    "print(predicted_test2)\n",
    "print(Y_test2)\n",
    "tpr_test2,tnr_test2=getTPTNRate(Y_test2,predicted_test2)\n",
    "print(\"true positive rate\",tpr_test2,\"true negative rate\",tnr_test2)\n",
    "\n",
    "print('Test')\n",
    "#predicted result based on input\n",
    "predicted_val=mypredictions.eval(feed_dict={X:X_val,is_training:False}, session=sess)\n",
    "print(predicted_val)\n",
    "print(Y_val)\n",
    "tpr_val,tnr_val=getTPTNRate(Y_val,predicted_val)\n",
    "print(\"true positive rate\",tpr_val,\"true negative rate\",tnr_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model is based on https://ac.els-cdn.com/S1877705817304289/1-s2.0-S1877705817304289-main.pdf?_tid=918bcd97-2bcc-4ee7-ae31-44ea555c002b&acdnat=1523213163_a7fcbb7625fce5e50f0e1ea690f38c09\n",
    "\n",
    "it has a structure of conv-conv-pool-conv-conv-pool-conv-conv-pool-fc-fc-softmax and each conv layer has a dropout to avoid overfitting\n",
    "\n",
    "however, this model's prediction that has close to 0 true positive rate and close to 1 true negative rate (3 epoch on 1000 training data)\n",
    "\n",
    "the model is improved by adding batch normalization after each conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "14 25 11 14\n",
      "Iteration 0: with minibatch training loss = 0.967,tpr of 0.5,tnr of 0.69 and accuracy of 0.61\n"
     ]
    }
   ],
   "source": [
    "class CNN_model:\n",
    "    def __init__(self,X_train,Y_train):\n",
    "        self.X_train=X_train\n",
    "        self.Y_train=Y_train\n",
    "        \n",
    "    def cnn_model_fn(self,X, labels, is_training):\n",
    "        #1-layer model: conv-pool-fc-fc-softmax with batch normalization \n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "        input_layer = tf.reshape(X, [-1, 227, 227, 1])\n",
    "\n",
    "#         conv1 = tf.layers.conv2d(\n",
    "#       inputs=input_layer,\n",
    "#       filters=32,\n",
    "#       kernel_size=[3, 3],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#                 #add dropout \n",
    "#         bn1act = tf.layers.batch_normalization(inputs=conv1, training=is_training)\n",
    "\n",
    "\n",
    "\n",
    "#         pool1 = tf.layers.max_pooling2d(inputs=bn1act, pool_size=[2, 2], strides=2)\n",
    "        conv2 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "        bn2act = tf.layers.batch_normalization(inputs=conv2, training=is_training)\n",
    "#                 #add dropout \n",
    "#         dropout_conv2 = tf.layers.dropout(\n",
    "#       inputs=bn2act, rate=0.3, training=is_training)\n",
    "\n",
    "\n",
    "\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=bn2act, pool_size=[2, 2], strides=2)\n",
    "       \n",
    "  # Flatten tensor into a batch of vectors\n",
    "        pool2_flat = tf.reshape(pool2, [-1, 113 * 113 * 32])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 128 neurons\n",
    "        dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "        bn2act = tf.layers.batch_normalization(inputs=dense, training=is_training)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept to avoid overfitting\n",
    "        dropout = tf.layers.dropout(\n",
    "      inputs=bn2act, rate=0.3, training=is_training)\n",
    "\n",
    "#   # Logits layer\n",
    "#   # Input Tensor Shape: [batch_size, 128]\n",
    "#   # Output Tensor Shape: [batch_size, 2]\n",
    "\n",
    "\n",
    "#   # Convolutional Layer #1\n",
    "#   # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "#   # Padding is added to preserve width and height.\n",
    "#   # Input Tensor Shape: [batch_size, 227, 227, 1]\n",
    "#   # Output Tensor Shape: [batch_size, 227, 227, 32]\n",
    "#         conv1 = tf.layers.conv2d(\n",
    "#       inputs=input_layer,\n",
    "#       filters=32,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn1act = tf.layers.batch_normalization(inputs=conv1, training=is_training)\n",
    "\n",
    "#   # Pooling Layer #1\n",
    "#   # First max pooling layer with a 2x2 filter and stride of 2\n",
    "#   # Input Tensor Shape: [batch_size, 227, 227, 32]\n",
    "#   # Output Tensor Shape: [batch_size, 113, 113, 32]\n",
    "#         pool1 = tf.layers.max_pooling2d(inputs=bn1act, pool_size=[2, 2], strides=2)\n",
    "       \n",
    "\n",
    "#   # Flatten tensor into a batch of vectors\n",
    "#   # Input Tensor Shape: [batch_size, 56, 56, 64]\n",
    "#   # Output Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "#     pool1_flat = tf.reshape(pool1, [-1, 113 * 113 * 8])\n",
    "\n",
    "#   # Dense Layer\n",
    "#   # Densely connected layer with 1024 neurons\n",
    "#   # Input Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "#   # Output Tensor Shape: [batch_size, 1024]\n",
    "#     dense = tf.layers.dense(inputs=pool1_flat, units=128, activation=tf.nn.relu)\n",
    "#     bn2act = tf.layers.batch_normalization(inputs=dense, training=is_training)\n",
    "\n",
    "#   # Add dropout operation; 0.6 probability that element will be kept\n",
    "#     dropout = tf.layers.dropout(\n",
    "#       inputs=bn2act, rate=0.4, training=is_training)\n",
    "\n",
    "#   # Logits layer\n",
    "#   # Input Tensor Shape: [batch_size, 1024]\n",
    "#   # Output Tensor Shape: [batch_size, 2]\n",
    "#     logits = tf.layers.dense(inputs=dropout, units=2,activation=None)\n",
    "\n",
    "#   # Convolutional Layer #2\n",
    "#   # Computes 64 features using a 5x5 filter.\n",
    "#   # Padding is added to preserve width and height.\n",
    "#   # Input Tensor Shape: [batch_size, 113, 113, 32]\n",
    "#   # Output Tensor Shape: [batch_size, 113, 113, 64]\n",
    "#         conv2 = tf.layers.conv2d(\n",
    "#       inputs=pool1,\n",
    "#       filters=64,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn2act = tf.layers.batch_normalization(inputs=conv2, training=is_training)\n",
    "\n",
    "#   # Pooling Layer #2\n",
    "#   # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "#   # Input Tensor Shape: [batch_size, 113, 113, 64]\n",
    "#   # Output Tensor Shape: [batch_size, 56, 56, 64]\n",
    "#         pool2 = tf.layers.max_pooling2d(inputs=bn2act, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "#         conv3 = tf.layers.conv2d(\n",
    "#       inputs=pool2,\n",
    "#       filters=64,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn3act = tf.layers.batch_normalization(inputs=conv3, training=is_training)\n",
    "\n",
    "#   # Pooling Layer #2\n",
    "#   # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "#   # Input Tensor Shape: [batch_size, 113, 113, 64]\n",
    "#   # Output Tensor Shape: [batch_size, 56, 56, 64]\n",
    "#         pool3 = tf.layers.max_pooling2d(inputs=bn3act, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#   # Flatten tensor into a batch of vectors\n",
    "#   # Input Tensor Shape: [batch_size, 56, 56, 64]\n",
    "#   # Output Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "#         pool3_flat = tf.reshape(pool3, [-1, 28 * 28 * 64])\n",
    "\n",
    "#   # Dense Layer\n",
    "#   # Densely connected layer with 1024 neurons\n",
    "#   # Input Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "#   # Output Tensor Shape: [batch_size, 1024]\n",
    "#         dense = tf.layers.dense(inputs=pool3_flat, units=1024, activation=tf.nn.relu)\n",
    "#         bn3act = tf.layers.batch_normalization(inputs=dense, training=is_training)\n",
    "\n",
    "#   # Add dropout operation; 0.6 probability that element will be kept\n",
    "#         dropout = tf.layers.dropout(\n",
    "#       inputs=bn3act, rate=0.4, training=is_training)\n",
    "\n",
    "\n",
    "#         conv1 = tf.layers.conv2d(\n",
    "#       inputs=input_layer,\n",
    "#       filters=16,\n",
    "#       kernel_size=[11, 11],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn1act = tf.layers.batch_normalization(inputs=conv1, training=is_training)\n",
    "#         dropout1 = tf.layers.dropout(\n",
    "#       inputs=bn1act, rate=0.2, training=is_training)\n",
    "#         conv2 = tf.layers.conv2d(\n",
    "#       inputs=dropout1,\n",
    "#       filters=32,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn2act = tf.layers.batch_normalization(inputs=conv2, training=is_training)\n",
    "#         dropout2 = tf.layers.dropout(\n",
    "#       inputs=bn2act, rate=0.3, training=is_training)\n",
    "#         pool1 = tf.layers.max_pooling2d(inputs=dropout2, pool_size=[3, 3], strides=2)\n",
    "#         conv3 = tf.layers.conv2d(\n",
    "#       inputs=pool1,\n",
    "#       filters=32,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn3act = tf.layers.batch_normalization(inputs=conv3, training=is_training)\n",
    "#         dropout3 = tf.layers.dropout(\n",
    "#       inputs=bn3act, rate=0.3, training=is_training)\n",
    "#         conv4 = tf.layers.conv2d(\n",
    "#       inputs=dropout3,\n",
    "#       filters=32,\n",
    "#       kernel_size=[3, 3],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn4act = tf.layers.batch_normalization(inputs=conv4, training=is_training)\n",
    "#         dropout4 = tf.layers.dropout(\n",
    "#       inputs=bn4act, rate=0.3, training=is_training)\n",
    "#         pool2 = tf.layers.max_pooling2d(inputs=dropout4, pool_size=[3, 3], strides=2)\n",
    "#         conv5 = tf.layers.conv2d(\n",
    "#       inputs=pool2,\n",
    "#       filters=32,\n",
    "#       kernel_size=[3, 3],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn5act = tf.layers.batch_normalization(inputs=conv5, training=is_training)\n",
    "#         dropout5 = tf.layers.dropout(\n",
    "#       inputs=bn5act, rate=0.3, training=is_training)\n",
    "#         conv6 = tf.layers.conv2d(\n",
    "#       inputs=dropout5,\n",
    "#       filters=32,\n",
    "#       kernel_size=[3, 3],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#         bn6act = tf.layers.batch_normalization(inputs=conv6, training=is_training)\n",
    "#         dropout6 = tf.layers.dropout(\n",
    "#       inputs=bn6act, rate=0.4, training=is_training)\n",
    "#         pool3 = tf.layers.max_pooling2d(inputs=dropout6, pool_size=[3, 3], strides=2)\n",
    "#         pool3_flat = tf.reshape(pool3, [-1, 27 * 27 * 32])\n",
    "#         dense1 = tf.layers.dense(inputs=pool3_flat, units=128, activation=tf.nn.relu)\n",
    "#         dropout6 = tf.layers.dropout(\n",
    "#       inputs=dense1, rate=0.5, training=is_training)\n",
    "#         logits = tf.layers.dense(inputs=dropout6, units=2,activation=None)\n",
    "\n",
    "        logits = tf.layers.dense(inputs=dropout, units=2,activation=None,name=\"logits\")\n",
    "        return logits\n",
    "\n",
    "\n",
    "#load model\n",
    "model=CNN_model(X_train,Y_train)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 227, 227],name=\"X\")\n",
    "y = tf.placeholder(tf.int64, [None],name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool,name=\"training\")\n",
    "\n",
    "y_out = model.cnn_model_fn(X,y,is_training)\n",
    "# print(y_out)\n",
    "# print(is_training)\n",
    "total_loss = tf.losses.softmax_cross_entropy(logits=y_out, onehot_labels=tf.one_hot(y,2))\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) #adam\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "print('Training')\n",
    "run_model_TF(sess,y_out,mean_loss,X_train,Y_train,6,64,100,train_step,True)\n",
    "#save the model\n",
    "#the training is over 4000 data(2000 pos 2000 neg) over 5 epoch\n",
    "# saver.save(sess, '../trained_model3/cnn_model')\n",
    "mypredictions=tf.argmax(y_out,1)\n",
    "print('Test')\n",
    "#predicted result based on input\n",
    "predicted_test=mypredictions.eval(feed_dict={X:X_test,is_training:False}, session=sess)\n",
    "print(predicted_test)\n",
    "print(Y_test)\n",
    "tpr_test,tnr_test=getTPTNRate(Y_test,predicted_test)\n",
    "print(\"true positive rate\",tpr_test,\"true negative rate\",tnr_test)\n",
    "\n",
    "# predicted_test2=mypredictions.eval(feed_dict={X:X_test2,is_training:False}, session=sess)\n",
    "# print(predicted_test2)\n",
    "# print(Y_test2)\n",
    "# tpr_test2,tnr_test2=getTPTNRate(Y_test2,predicted_test2)\n",
    "# print(\"true positive rate\",tpr_test2,\"true negative rate\",tnr_test2)\n",
    "\n",
    "print('Test')\n",
    "#predicted result based on input\n",
    "predicted_val=mypredictions.eval(feed_dict={X:X_val,is_training:False}, session=sess)\n",
    "print(predicted_val)\n",
    "print(Y_val)\n",
    "tpr_val,tnr_val=getTPTNRate(Y_val,predicted_val)\n",
    "print(\"true positive rate\",tpr_val,\"true negative rate\",tnr_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "80\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cb7742f9894b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print(predicted_test[i,0]+predicted_test[i,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# mypredictions=y_out\n",
    "print('Test')\n",
    "#predicted result based on input\n",
    "predicted_test=mypredictions.eval(feed_dict={X:X_test,is_training:False}, session=sess)\n",
    "print(predicted_test)\n",
    "result=np.zeros(Y_test.size)\n",
    "print(predicted_test.size)\n",
    "print(predicted_test[0,0])\n",
    "for i in range(int(predicted_test.size/2)):\n",
    "#     print(predicted_test[i,0]+predicted_test[i,1])\n",
    "\n",
    "    if abs(predicted_test[i,0]+predicted_test[i,1])<0.5:\n",
    "        print(2)\n",
    "        result[i]=1\n",
    "    else:\n",
    "        result[i]=np.argmax(predicted_test[i])\n",
    "print(result)\n",
    "        \n",
    "        \n",
    "print(Y_test)\n",
    "tpr_test,tnr_test=getTPTNRate(Y_test,result)\n",
    "print(\"true positive rate\",tpr_test,\"true negative rate\",tnr_test)\n",
    "\n",
    "# # predicted_test2=mypredictions.eval(feed_dict={X:X_test2,is_training:False}, session=sess)\n",
    "# # print(predicted_test2)\n",
    "# # print(Y_test2)\n",
    "# # tpr_test2,tnr_test2=getTPTNRate(Y_test2,predicted_test2)\n",
    "# # print(\"true positive rate\",tpr_test2,\"true negative rate\",tnr_test2)\n",
    "\n",
    "# print('Test')\n",
    "# #predicted result based on input\n",
    "# predicted_val=mypredictions.eval(feed_dict={X:X_val,is_training:False}, session=sess)\n",
    "# print(predicted_val)\n",
    "# print(Y_val)\n",
    "# tpr_val,tnr_val=getTPTNRate(Y_val,predicted_val)\n",
    "# print(\"true positive rate\",tpr_val,\"true negative rate\",tnr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unused\n",
    "def cnn_model_fn_old(X, labels, is_training):\n",
    "\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(X, [-1, 227, 227, 1])\n",
    "    \n",
    "    \n",
    "#     conv1 = tf.layers.conv2d(\n",
    "#       inputs=input_layer,\n",
    "#       filters=16,\n",
    "#       kernel_size=[11, 11],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#     bn1act = tf.layers.batch_normalization(inputs=conv1, training=is_training)\n",
    "#     dropout1 = tf.layers.dropout(\n",
    "#       inputs=bn1act, rate=0.2, training=is_training)\n",
    "#     conv2 = tf.layers.conv2d(\n",
    "#       inputs=dropout1,\n",
    "#       filters=32,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#     bn2act = tf.layers.batch_normalization(inputs=conv2, training=is_training)\n",
    "#     dropout2 = tf.layers.dropout(\n",
    "#       inputs=bn2act, rate=0.3, training=is_training)\n",
    "#     pool1 = tf.layers.max_pooling2d(inputs=dropout2, pool_size=[3, 3], strides=2)\n",
    "#     conv3 = tf.layers.conv2d(\n",
    "#       inputs=pool1,\n",
    "#       filters=32,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#     bn3act = tf.layers.batch_normalization(inputs=conv3, training=is_training)\n",
    "#     dropout3 = tf.layers.dropout(\n",
    "#       inputs=bn3act, rate=0.3, training=is_training)\n",
    "#     conv4 = tf.layers.conv2d(\n",
    "#       inputs=dropout3,\n",
    "#       filters=32,\n",
    "#       kernel_size=[3, 3],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#     bn4act = tf.layers.batch_normalization(inputs=conv4, training=is_training)\n",
    "#     dropout4 = tf.layers.dropout(\n",
    "#       inputs=bn4act, rate=0.3, training=is_training)\n",
    "#     pool2 = tf.layers.max_pooling2d(inputs=dropout4, pool_size=[3, 3], strides=2)\n",
    "#     conv5 = tf.layers.conv2d(\n",
    "#       inputs=pool2,\n",
    "#       filters=32,\n",
    "#       kernel_size=[3, 3],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#     bn5act = tf.layers.batch_normalization(inputs=conv5, training=is_training)\n",
    "#     dropout5 = tf.layers.dropout(\n",
    "#       inputs=bn5act, rate=0.3, training=is_training)\n",
    "#     conv6 = tf.layers.conv2d(\n",
    "#       inputs=dropout5,\n",
    "#       filters=32,\n",
    "#       kernel_size=[3, 3],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#     bn6act = tf.layers.batch_normalization(inputs=conv6, training=is_training)\n",
    "#     dropout6 = tf.layers.dropout(\n",
    "#       inputs=bn6act, rate=0.4, training=is_training)\n",
    "#     pool3 = tf.layers.max_pooling2d(inputs=dropout6, pool_size=[3, 3], strides=2)\n",
    "#     pool3_flat = tf.reshape(pool3, [-1, 27 * 27 * 32])\n",
    "#     dense1 = tf.layers.dense(inputs=pool3_flat, units=128, activation=tf.nn.relu)\n",
    "#     dropout6 = tf.layers.dropout(\n",
    "#       inputs=dense1, rate=0.5, training=is_training)\n",
    "#     logits = tf.layers.dense(inputs=dropout6, units=2,activation=None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 227, 227, 1]\n",
    "  # Output Tensor Shape: [batch_size, 227, 227, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=8,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    bn1act = tf.layers.batch_normalization(inputs=conv1, training=is_training)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 227, 227, 32]\n",
    "  # Output Tensor Shape: [batch_size, 113, 113, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=bn1act, pool_size=[2, 2], strides=2)\n",
    "       \n",
    "\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 56, 56, 64]\n",
    "  # Output Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "    pool1_flat = tf.reshape(pool1, [-1, 113 * 113 * 8])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool1_flat, units=128, activation=tf.nn.relu)\n",
    "    bn2act = tf.layers.batch_normalization(inputs=dense, training=is_training)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=bn2act, rate=0.4, training=is_training)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 2]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2,activation=None)\n",
    "\n",
    "#   # Convolutional Layer #2\n",
    "#   # Computes 64 features using a 5x5 filter.\n",
    "#   # Padding is added to preserve width and height.\n",
    "#   # Input Tensor Shape: [batch_size, 113, 113, 32]\n",
    "#   # Output Tensor Shape: [batch_size, 113, 113, 64]\n",
    "#     conv2 = tf.layers.conv2d(\n",
    "#       inputs=pool1,\n",
    "#       filters=64,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#     bn2act = tf.layers.batch_normalization(inputs=conv2, training=is_training)\n",
    "\n",
    "#   # Pooling Layer #2\n",
    "#   # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "#   # Input Tensor Shape: [batch_size, 113, 113, 64]\n",
    "#   # Output Tensor Shape: [batch_size, 56, 56, 64]\n",
    "#     pool2 = tf.layers.max_pooling2d(inputs=bn2act, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "#     conv3 = tf.layers.conv2d(\n",
    "#       inputs=pool2,\n",
    "#       filters=64,\n",
    "#       kernel_size=[5, 5],\n",
    "#       padding=\"same\",\n",
    "#       activation=tf.nn.relu)\n",
    "#     bn3act = tf.layers.batch_normalization(inputs=conv3, training=is_training)\n",
    "\n",
    "#   # Pooling Layer #2\n",
    "#   # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "#   # Input Tensor Shape: [batch_size, 113, 113, 64]\n",
    "#   # Output Tensor Shape: [batch_size, 56, 56, 64]\n",
    "#     pool3 = tf.layers.max_pooling2d(inputs=bn3act, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#   # Flatten tensor into a batch of vectors\n",
    "#   # Input Tensor Shape: [batch_size, 56, 56, 64]\n",
    "#   # Output Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "#     pool3_flat = tf.reshape(pool3, [-1, 56 * 56 * 64])\n",
    "\n",
    "#   # Dense Layer\n",
    "#   # Densely connected layer with 1024 neurons\n",
    "#   # Input Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "#   # Output Tensor Shape: [batch_size, 1024]\n",
    "#     dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "#     bn3act = tf.layers.batch_normalization(inputs=dense, training=is_training)\n",
    "\n",
    "#   # Add dropout operation; 0.6 probability that element will be kept\n",
    "#     dropout = tf.layers.dropout(\n",
    "#       inputs=bn3act, rate=0.4, training=is_training)\n",
    "\n",
    "#   # Logits layer\n",
    "#   # Input Tensor Shape: [batch_size, 1024]\n",
    "#   # Output Tensor Shape: [batch_size, 2]\n",
    "#     logits = tf.layers.dense(inputs=dropout, units=2,activation=None)\n",
    "    return logits\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with 1000 training data entries 10 epoch, validate with 50 data entries and test with 100 data entries:\n",
    "\n",
    "Validation\n",
    "\n",
    "Epoch 1, Overall loss = 0.247 and accuracy of 0.94\n",
    "\n",
    "Test\n",
    "\n",
    "Epoch 1, Overall loss = 0.298 and accuracy of 0.89\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new test data\n",
    "## conv-pool-fc(128)-fc-softmax model; training 5 epoches over 4000 data\n",
    "true positive rate 0.6 true negative rate 1.0 (osillating loss)\n",
    "## conv-pool-fc(1024)-fc-softmax model; training 5 epoches over 4000 data\n",
    "true positive rate 0.975 true negative rate 0.975\n",
    "true positive rate 0.5 true negative rate 1.0 (osillating loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## conv-pool-fc-fc-softmax model; training 3 epoches over 2000 data\n",
    "validation:true positive rate 0.8 true negative rate 0.975\n",
    "\n",
    "test:true positive rate 0.85 true negative rate 0.99\n",
    "\n",
    "## conv-pool-fc(1024)-fc-softmax model; training 4 epoches over 2000 data\n",
    "validation:true positive rate 0.875 true negative rate 0.975\n",
    "\n",
    "test:true positive rate 0.89 true negative rate 0.96\n",
    "\n",
    "## conv-pool-fc(512)-fc-softmax model; training 4 epoches over 2000 data\n",
    "validation:true positive rate 0.775 true negative rate 0.9\n",
    "\n",
    "test:true positive rate 0.88 true negative rate 0.92\n",
    "\n",
    "## conv-pool-fc(128)-fc-softmax model; training 4 epoches over 2000 data\n",
    "validation:true positive rate 0.925 true negative rate 0.95/true positive rate 0.85 true negative rate 0.925\n",
    "\n",
    "test:true positive rate 0.92 true negative rate 0.99/true positive rate 0.83 true negative rate 0.98\n",
    "\n",
    "## conv-pool-fc(64)-fc-softmax model; training 4 epoches over 2000 data\n",
    "validation:true positive rate 0.8 true negative rate 0.975\n",
    "\n",
    "test:true positive rate 0.88 true negative rate 0.97\n",
    "\n",
    "## conv-pool-fc-fc-softmax model; training 4 epoches over 4000 data\n",
    "validation:true positive rate 0.925 true negative rate 1.0\n",
    "\n",
    "test:true positive rate 0.94 true negative rate 0.98\n",
    "\n",
    "## conv-pool-conv-pool-conv-pool-fc-fc-softmax model; training 3 epoches over 1000 data\n",
    "\n",
    "validation:true positive rate 0.9 true negative rate 0.95\n",
    "\n",
    "test:true positive rate 0.91 true negative rate 0.96\n",
    "\n",
    "## conv-pool-conv-pool-conv-pool-fc-fc-softmax model; training 4 epoches over 2000 data\n",
    "validation:true positive rate 0.975 true negative rate 0.95\n",
    "\n",
    "test:true positive rate 0.95 true negative rate 0.93\n",
    "\n",
    "## conv-conv-pool-conv-conv-pool-conv-conv-pool-fc-fc-softmax model; training 3 epoches over 1000 data\n",
    "\n",
    "validation: true positive rate 0.9 true negative rate 0.95\n",
    "\n",
    "test: true positive rate 0.96 true negative rate 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
