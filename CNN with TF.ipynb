{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab as pl\n",
    "import math\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess the image by adding filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(X, labels, is_training):\n",
    "\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(X, [-1, 227, 227, 1])\n",
    "\n",
    "  # Convolutional Layer #1\n",
    "  # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 227, 227, 1]\n",
    "  # Output Tensor Shape: [batch_size, 227, 227, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    bn1act = tf.layers.batch_normalization(inputs=conv1, training=is_training)\n",
    "\n",
    "  # Pooling Layer #1\n",
    "  # First max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 227, 227, 32]\n",
    "  # Output Tensor Shape: [batch_size, 113, 113, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=bn1act, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Convolutional Layer #2\n",
    "  # Computes 64 features using a 5x5 filter.\n",
    "  # Padding is added to preserve width and height.\n",
    "  # Input Tensor Shape: [batch_size, 113, 113, 32]\n",
    "  # Output Tensor Shape: [batch_size, 113, 113, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    bn2act = tf.layers.batch_normalization(inputs=conv2, training=is_training)\n",
    "\n",
    "  # Pooling Layer #2\n",
    "  # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "  # Input Tensor Shape: [batch_size, 113, 113, 64]\n",
    "  # Output Tensor Shape: [batch_size, 56, 56, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=bn2act, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Flatten tensor into a batch of vectors\n",
    "  # Input Tensor Shape: [batch_size, 56, 56, 64]\n",
    "  # Output Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 56 * 56 * 64])\n",
    "\n",
    "  # Dense Layer\n",
    "  # Densely connected layer with 1024 neurons\n",
    "  # Input Tensor Shape: [batch_size, 56 * 56 * 64]\n",
    "  # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    bn3act = tf.layers.batch_normalization(inputs=dense, training=is_training)\n",
    "\n",
    "  # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=bn3act, rate=0.4, training=is_training)\n",
    "\n",
    "  # Logits layer\n",
    "  # Input Tensor Shape: [batch_size, 1024]\n",
    "  # Output Tensor Shape: [batch_size, 2]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2,activation=None)\n",
    "    return logits\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#   predictions = {\n",
    "#       # Generate predictions (for PREDICT and EVAL mode)\n",
    "#       \"classes\": tf.argmax(input=logits, axis=1),\n",
    "#       # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "#       # `logging_hook`.\n",
    "#       \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "#   }\n",
    "#   if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "#     return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "#   loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "#   # Configure the Training Op (for TRAIN mode)\n",
    "#   if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "#     train_op = optimizer.minimize(\n",
    "#         loss=loss,\n",
    "#         global_step=tf.train.get_global_step())\n",
    "#     return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "#   # Add evaluation metrics (for EVAL mode)\n",
    "#   eval_metric_ops = {\n",
    "#       \"accuracy\": tf.metrics.accuracy(\n",
    "#           labels=labels, predictions=predictions[\"classes\"])}\n",
    "#   return tf.estimator.EstimatorSpec(\n",
    "#       mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "#   train_data = mnist.train.images  # Returns np.array\n",
    "#   train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "#   eval_data = mnist.test.images  # Returns np.array\n",
    "#   eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "#   # Create the Estimator\n",
    "#   mnist_classifier = tf.estimator.Estimator(\n",
    "#       model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\n",
    "\n",
    "#   # Set up logging for predictions\n",
    "#   # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "#   tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "#   logging_hook = tf.train.LoggingTensorHook(\n",
    "#       tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "#   # Train the model\n",
    "#   train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#       x={\"x\": train_data},\n",
    "#       y=train_labels,\n",
    "#       batch_size=100,\n",
    "#       num_epochs=None,\n",
    "#       shuffle=True)\n",
    "#   mnist_classifier.train(\n",
    "#       input_fn=train_input_fn,\n",
    "#       steps=20000,\n",
    "#       hooks=[logging_hook])\n",
    "\n",
    "#   # Evaluate the model and print results\n",
    "#   eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#       x={\"x\": eval_data},\n",
    "#       y=eval_labels,\n",
    "#       num_epochs=1,\n",
    "#       shuffle=False)\n",
    "#   eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "#   print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_grayscale(image, beta=0.75, image_id_k = 1, A_kp=None):\n",
    "    N,M = image.shape\n",
    "    k = image_id_k\n",
    "    \n",
    "    if image_id_k > 1 and  A_kp is None :\n",
    "        print(\"please define A_kp as this is not the first image\")\n",
    "    elif image_id_k == 1:\n",
    "        A_kp = np.zeros((N,M))\n",
    "        \n",
    "    alpha = beta*(k-1)/k\n",
    "    a = np.mean(image, axis=0)\n",
    "    A_k = alpha*A_kp + (1-alpha)*np.tile(a,(N,1))\n",
    "    image = 128*(image/A_k)\n",
    "    return image, A_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# features={\"x_train\":X_train}\n",
    "# labels=Y_train\n",
    "# def train_input_fn(features, labels, batch_size):\n",
    "#     \"\"\"An input function for training\"\"\"\n",
    "#     # Convert the inputs to a Dataset.\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "#     # Shuffle, repeat, and batch the examples.\n",
    "#     dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "#     # Build the Iterator, and return the read end of the pipeline.\n",
    "#     return dataset.make_one_shot_iterator().get_next()\n",
    "# features_result,labels_result=train_input_fn(features, labels, 10)    \n",
    "\n",
    "\n",
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss, correct_prediction, accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        # keep track of accuracy\n",
    "        correct = 0\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            corr = np.array(corr).astype(np.float32)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "            \n",
    "    return total_loss,total_correct,losses\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 227, 227])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out = cnn_model_fn(X,y,is_training)\n",
    "total_loss = tf.losses.softmax_cross_entropy(logits=y_out, onehot_labels=tf.one_hot(y,2))\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4) #adam\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "# txt = '../Negative/00001.jpg'\n",
    "# img = cv2.imread(txt, 0).astype(np.float32)\n",
    "# print(img2.shape)\n",
    "# plt.figure(figsize=(10,5))\n",
    "# plt.subplot(121),plt.imshow(img, cmap='gray')\n",
    "# plt.title('Original'),plt.xticks([]),plt.yticks([])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# print('Training')\n",
    "# run_model(sess,y_out,mean_loss,X_train,Y_train,10,64,100,train_step,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Validation')\n",
    "# run_model(sess,y_out,mean_loss,X_val,Y_val,1,64)\n",
    "# print('Test')\n",
    "# run_model(sess,y_out,mean_loss,X_test,Y_test,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with 1000 training data entries 10 epoch, validate with 50 data entries and test with 100 data entries:\n",
    "\n",
    "Validation\n",
    "\n",
    "Epoch 1, Overall loss = 0.247 and accuracy of 0.94\n",
    "\n",
    "Test\n",
    "\n",
    "Epoch 1, Overall loss = 0.298 and accuracy of 0.89\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: test the accuracy on positive and negative \n",
    "## add filter and train again (with maybe 500 data entries for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model_TF(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    predictions=tf.argmax(predict,1)\n",
    "    actuals=y \n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    zeros_like_actuals = tf.zeros_like(actuals)\n",
    "    ones_like_predictions = tf.ones_like(predictions)\n",
    "    zeros_like_predictions = tf.zeros_like(predictions)\n",
    "    tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "    tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "    fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "    fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    \n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    \n",
    "    training_now = (training is not None)\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss, tp_op, tn_op, fp_op, fn_op]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    # keep track of losses\n",
    "    losses = []\n",
    "    tprs=[]\n",
    "    tnrs=[]\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # shuffle indicies\n",
    "        np.random.shuffle(train_indicies)\n",
    "        total_tp=0\n",
    "        total_tn=0\n",
    "        total_fp=0\n",
    "        total_fn=0\n",
    "\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, tp, tn, fp, fn = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "            if fn is None:\n",
    "                fn=actual_batch_size-tp-tn-fp\n",
    "   \n",
    "            print(tp,tn,fp,fn)\n",
    "            total_tp+=tp\n",
    "            total_tn+=tn\n",
    "            total_fp+=fp\n",
    "            total_fn+=fn\n",
    "            tpr = float(tp)/(float(tp) + float(fn))\n",
    "            fpr = float(fp)/(float(tp) + float(fn))\n",
    "            tnr=float(tn)/(float(tn) + float(fp))\n",
    "            accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "            \n",
    "            recall = tpr\n",
    "            precision = float(tp)/(float(tp) + float(fp))\n",
    "            f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            tprs.append(tpr)\n",
    "            tnrs.append(tnr)\n",
    "            print(len(tprs),tprs)\n",
    "            print(len(tnrs),tnrs)\n",
    "\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g},tpr of {2:.2g} and tnr of {3:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,tpr,tnr))\n",
    "            iter_cnt += 1\n",
    "        total_tpr = total_tp/(total_tp+total_fn)\n",
    "        total_tnr=total_tn/(total_tn+total_fp)\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "\n",
    "\n",
    "        print(\"Epoch {3}, Overall loss = {0:.3g} tpr of {1:.3g} and tnr of {2:.3g}\"\\\n",
    "              .format(total_loss,total_tpr,total_tnr,e+1))\n",
    "        \n",
    "    if plot_losses:\n",
    "        plt.plot(losses)\n",
    "        plt.grid(True)\n",
    "        plt.title('Epoch {} Loss'.format(e+1))\n",
    "        plt.xlabel('minibatch number')\n",
    "        plt.ylabel('minibatch loss')\n",
    "        plt.show()\n",
    "            \n",
    "    return total_loss,total_tpr,total_tnr,losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "#train 500 negatives and 500 positives\n",
    "#use 50 negatives and 50 positives to validate\n",
    "#100 negatives and 100 positives to test\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_test=[]\n",
    "Y_test=[] \n",
    "X_val=[]\n",
    "Y_val=[]  \n",
    "\n",
    "for i in range(1,501):\n",
    "    j  = i + 1600\n",
    "    txt = '../Positive/0'+str(j).zfill(4)+'.jpg'\n",
    "    img = cv2.imread(txt, 0).astype(np.float32)\n",
    "    X_train.append(img)\n",
    "    Y_train.append(1)\n",
    "\n",
    "for i in range(1,501):\n",
    "    j  = i + 1600\n",
    "    txt = '../Negative/0'+str(j).zfill(4)+'.jpg'\n",
    "    img = cv2.imread(txt, 0).astype(np.float32)\n",
    "    X_train.append(img)\n",
    "    Y_train.append(0)\n",
    "    \n",
    "for i in range(1,41):\n",
    "    j  = i + 1000\n",
    "    txt = '../Positive/1'+str(i).zfill(4)+'_1.jpg'\n",
    "    img = cv2.imread(txt, 0).astype(np.float32)\n",
    "    X_val.append(img)\n",
    "    Y_val.append(1)\n",
    "\n",
    "for i in range(1,41):\n",
    "    j  = i + 1000\n",
    "    txt = '../Negative/1'+str(i).zfill(4)+'.jpg'\n",
    "    img = cv2.imread(txt, 0).astype(np.float32)\n",
    "    X_val.append(img)\n",
    "    Y_val.append(0)\n",
    "\n",
    "   \n",
    "for i in range(1,101):\n",
    "    j  = i + 1000\n",
    "    txt = '../Positive/1'+str(j).zfill(4)+'_1.jpg'\n",
    "    img = cv2.imread(txt, 0).astype(np.float32)\n",
    "    X_test.append(img)\n",
    "    Y_test.append(1)\n",
    "\n",
    "for i in range(1,101):\n",
    "    j  = i + 1000\n",
    "    txt = '../Negative/1'+str(j).zfill(4)+'.jpg'\n",
    "    img = cv2.imread(txt, 0).astype(np.float32)\n",
    "    X_test.append(img)\n",
    "    Y_test.append(0)\n",
    "\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test=np.asarray(Y_test)    \n",
    "X_val=np.asarray(X_val)\n",
    "Y_val=np.asarray(Y_val)\n",
    "X_train=np.asarray(X_train)\n",
    "Y_train=np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "15.0 10.0 21.0 18.0\n",
      "1 [0.45454545454545453]\n",
      "1 [0.3225806451612903]\n",
      "Iteration 0: with minibatch training loss = 1.31,tpr of 0.45 and tnr of 0.32\n",
      "25.0 33.0 1.0 5.0\n",
      "2 [0.45454545454545453, 0.8333333333333334]\n",
      "2 [0.3225806451612903, 0.9705882352941176]\n",
      "28.0 26.0 1.0 9.0\n",
      "3 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568]\n",
      "3 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629]\n",
      "25.0 29.0 5.0 5.0\n",
      "4 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334]\n",
      "4 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882]\n",
      "32.0 24.0 2.0 6.0\n",
      "5 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947]\n",
      "5 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231]\n",
      "25.0 30.0 1.0 8.0\n",
      "6 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576]\n",
      "6 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871]\n",
      "25.0 30.0 4.0 5.0\n",
      "7 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334]\n",
      "7 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706]\n",
      "29.0 29.0 1.0 5.0\n",
      "8 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882]\n",
      "8 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667]\n",
      "24.0 34.0 4.0 2.0\n",
      "9 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231]\n",
      "9 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632]\n",
      "27.0 32.0 2.0 3.0\n",
      "10 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9]\n",
      "10 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353]\n",
      "23.0 35.0 4.0 2.0\n",
      "11 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92]\n",
      "11 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975]\n",
      "29.0 29.0 1.0 5.0\n",
      "12 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882]\n",
      "12 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667]\n",
      "26.0 30.0 1.0 7.0\n",
      "13 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878]\n",
      "13 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871]\n",
      "31.0 23.0 0.0 10.0\n",
      "14 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098]\n",
      "14 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0]\n",
      "21.0 38.0 1.0 4.0\n",
      "15 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84]\n",
      "15 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743]\n",
      "18.0 19.0 0.0 3.0\n",
      "16 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571]\n",
      "16 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0]\n",
      "Epoch 1, Overall loss = 0.578 tpr of 0.806 and tnr of 0.902\n",
      "30.0 34.0 0.0 0.0\n",
      "17 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0]\n",
      "17 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0]\n",
      "27.0 33.0 1.0 3.0\n",
      "18 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9]\n",
      "18 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176]\n",
      "30.0 32.0 0.0 2.0\n",
      "19 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375]\n",
      "19 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0]\n",
      "27.0 33.0 0.0 4.0\n",
      "20 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839]\n",
      "20 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.0 32.0 0.0 3.0\n",
      "21 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625]\n",
      "21 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0]\n",
      "31.0 30.0 0.0 3.0\n",
      "22 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529]\n",
      "22 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0]\n",
      "30.0 33.0 0.0 1.0\n",
      "23 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871]\n",
      "23 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "33.0 27.0 0.0 4.0\n",
      "24 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919]\n",
      "24 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "24.0 38.0 1.0 1.0\n",
      "25 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96]\n",
      "25 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743]\n",
      "27.0 37.0 0.0 0.0\n",
      "26 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0]\n",
      "26 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0]\n",
      "31.0 30.0 0.0 3.0\n",
      "27 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529]\n",
      "27 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0]\n",
      "29.0 32.0 1.0 2.0\n",
      "28 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419]\n",
      "28 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697]\n",
      "25.0 35.0 1.0 3.0\n",
      "29 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429]\n",
      "29 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222]\n",
      "34.0 22.0 1.0 7.0\n",
      "30 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268]\n",
      "30 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348]\n",
      "30.0 32.0 0.0 2.0\n",
      "31 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375]\n",
      "31 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.0 15.0 0.0 4.0\n",
      "32 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84]\n",
      "32 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0]\n",
      "Epoch 2, Overall loss = 0.739 tpr of 0.916 and tnr of 0.99\n",
      "28.0 35.0 1.0 0.0\n",
      "33 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0]\n",
      "33 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222]\n",
      "25.0 39.0 0.0 0.0\n",
      "34 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0]\n",
      "34 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0]\n",
      "32.0 31.0 0.0 1.0\n",
      "35 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697]\n",
      "35 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0]\n",
      "29.0 33.0 1.0 1.0\n",
      "36 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667]\n",
      "36 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176]\n",
      "25.0 37.0 2.0 0.0\n",
      "37 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0]\n",
      "37 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487]\n",
      "28.0 33.0 2.0 1.0\n",
      "38 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104]\n",
      "38 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428]\n",
      "32.0 30.0 0.0 2.0\n",
      "39 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353]\n",
      "39 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0 32.0 0.0 1.0\n",
      "40 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875]\n",
      "40 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0]\n",
      "34.0 27.0 0.0 3.0\n",
      "41 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875, 0.918918918918919]\n",
      "41 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0, 1.0]\n",
      "35.0 25.0 1.0 3.0\n",
      "42 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875, 0.918918918918919, 0.9210526315789473]\n",
      "42 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0, 1.0, 0.9615384615384616]\n",
      "33.0 29.0 0.0 2.0\n",
      "43 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875, 0.918918918918919, 0.9210526315789473, 0.9428571428571428]\n",
      "43 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0, 1.0, 0.9615384615384616, 1.0]\n",
      "31.0 31.0 0.0 2.0\n",
      "44 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875, 0.918918918918919, 0.9210526315789473, 0.9428571428571428, 0.9393939393939394]\n",
      "44 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0, 1.0, 0.9615384615384616, 1.0, 1.0]\n",
      "34.0 26.0 0.0 4.0\n",
      "45 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875, 0.918918918918919, 0.9210526315789473, 0.9428571428571428, 0.9393939393939394, 0.8947368421052632]\n",
      "45 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0, 1.0, 0.9615384615384616, 1.0, 1.0, 1.0]\n",
      "29.0 35.0 0.0 0.0\n",
      "46 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875, 0.918918918918919, 0.9210526315789473, 0.9428571428571428, 0.9393939393939394, 0.8947368421052632, 1.0]\n",
      "46 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0, 1.0, 0.9615384615384616, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0 29.0 0.0 3.0\n",
      "47 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875, 0.918918918918919, 0.9210526315789473, 0.9428571428571428, 0.9393939393939394, 0.8947368421052632, 1.0, 0.9142857142857143]\n",
      "47 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0, 1.0, 0.9615384615384616, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "19.0 21.0 0.0 0.0\n",
      "48 [0.45454545454545453, 0.8333333333333334, 0.7567567567567568, 0.8333333333333334, 0.8421052631578947, 0.7575757575757576, 0.8333333333333334, 0.8529411764705882, 0.9230769230769231, 0.9, 0.92, 0.8529411764705882, 0.7878787878787878, 0.7560975609756098, 0.84, 0.8571428571428571, 1.0, 0.9, 0.9375, 0.8709677419354839, 0.90625, 0.9117647058823529, 0.967741935483871, 0.8918918918918919, 0.96, 1.0, 0.9117647058823529, 0.9354838709677419, 0.8928571428571429, 0.8292682926829268, 0.9375, 0.84, 1.0, 1.0, 0.9696969696969697, 0.9666666666666667, 1.0, 0.9655172413793104, 0.9411764705882353, 0.96875, 0.918918918918919, 0.9210526315789473, 0.9428571428571428, 0.9393939393939394, 0.8947368421052632, 1.0, 0.9142857142857143, 1.0]\n",
      "48 [0.3225806451612903, 0.9705882352941176, 0.9629629629629629, 0.8529411764705882, 0.9230769230769231, 0.967741935483871, 0.8823529411764706, 0.9666666666666667, 0.8947368421052632, 0.9411764705882353, 0.8974358974358975, 0.9666666666666667, 0.967741935483871, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9705882352941176, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.9696969696969697, 0.9722222222222222, 0.9565217391304348, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 0.9705882352941176, 0.9487179487179487, 0.9428571428571428, 1.0, 1.0, 1.0, 0.9615384615384616, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Epoch 3, Overall loss = 0.806 tpr of 0.954 and tnr of 0.986\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAHwCAYAAADuJ7gwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130c64208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "35.0 0.0 29.0 0.0\n",
      "1 [1.0]\n",
      "1 [0.0]\n",
      "5.0 0.0 11.0 0.0\n",
      "2 [1.0, 1.0]\n",
      "2 [0.0, 0.0]\n",
      "Epoch 1, Overall loss = 1.68 tpr of 1 and tnr of 0\n",
      "Test\n",
      "32.0 0.0 32.0 0.0\n",
      "1 [1.0]\n",
      "1 [0.0]\n",
      "31.0 0.0 33.0 0.0\n",
      "2 [1.0, 1.0]\n",
      "2 [0.0, 0.0]\n",
      "35.0 0.0 29.0 0.0\n",
      "3 [1.0, 1.0, 1.0]\n",
      "3 [0.0, 0.0, 0.0]\n",
      "2.0 0.0 6.0 0.0\n",
      "4 [1.0, 1.0, 1.0, 1.0]\n",
      "4 [0.0, 0.0, 0.0, 0.0]\n",
      "Epoch 1, Overall loss = 1.81 tpr of 1 and tnr of 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.8147804069519042,\n",
       " 1.0,\n",
       " 0.0,\n",
       " [116.50729370117188,\n",
       "  118.82865905761719,\n",
       "  104.35404968261719,\n",
       "  23.266078948974609])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model_TF(sess,y_out,mean_loss,X_train,Y_train,3,64,100,train_step,True)\n",
    "print('Validation')\n",
    "run_model_TF(sess,y_out,mean_loss,X_val,Y_val,1,64)\n",
    "print('Test')\n",
    "run_model_TF(sess,y_out,mean_loss,X_test,Y_test,1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
